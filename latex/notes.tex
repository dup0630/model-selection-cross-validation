\documentclass[12pt, letter paper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,geometry,fancyhdr,graphicx,xcolor,biblatex}
%\usepackage{minted}
%\usepackage[most]{tcolorbox}

\addbibresource{citations.bib}
\geometry{letterpaper, margin=1in}

% For indicator functions:
\DeclareMathAlphabet{\mathmybb}{U}{bbold}{m}{n}
\newcommand{\1}{\mathmybb{1}}

%\newtcolorbox[auto counter, number within=section]{proposition}[2][Proposition]{%
%  colframe=blue!50!black,
%  colback=blue!10,
%  coltitle=white,
%  fonttitle=\bfseries,
%  title=#1~\thetcbcounter\ifx#2\empty\else~(#2)\fi}
%
%\newtcolorbox[auto counter, number within=section]{definition}{%
%  fonttitle=\bfseries,
%  title=Definition~\thetcbcounter,
%  label={def:\thetcbcounter}}


\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Borel}[1]{\mathcal{B}\paren{#1}}
\newcommand{\0}{\emptyset}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ep}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\paren}[1]{\left(#1 \right)}
\newcommand{\sqbr}[1]{\left[#1 \right]}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\norm}[1]{|\hspace{-1pt}|#1 |\hspace{-1pt}|}
\newcommand{\normsq}[1]{\norm{#1}^{2}}
\newcommand{\ind}[1]{\mathmybb{1}_{#1}}
\newcommand{\data}{\mathcal{D}_{n}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\aseq}{\stackrel{\mathrm{a.s.}}{=}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\Cov}[1]{\mathrm{Cov}\paren{#1}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\Loss}[1]{L_{n}\paren{#1}}
\newcommand{\Rhat}[2]{\hat{R}_{n, #1}\paren{#2}}
\newcommand{\alphahat}[1]{\hat{\alpha}^{#1}}
\newcommand{\lev}{h_{ii,\alpha}}
\newcommand{\loocv}[1]{\hat{R}^{(1)}_{n}\paren{#1}}
\newcommand{\op}[1]{o_{\prob}\paren{#1}}
\newcommand{\Op}[1]{O_{\prob}\paren{#1}}
\newcommand{\sigmahat}{\hat{\sigma}^{2}_{n}}
\newcommand{\fhat}[2]{\hat{f}_{#1, #2}}
\newcommand{\ftilde}[1]{\tilde{f}_{#1}}
\newcommand{\lambdahat}[1]{\hat{\lambda}_{#1}}
\newcommand{\blambdahat}{\boldsymbol{\hat{\lambda}}}

\title{Project: Cross-validation for model selection

(notes on papers)}
\author{Diego Urdapilleta de la Parra}
\begin{document}

\maketitle

\textcolor{red}{Note: I removed the boxes to keep the proposition enumeration consistent, and to make the text easier to read.}

\tableofcontents

\newpage
\section{CV for Linear Model Selection}
\subsection{Setup and preliminary results}
Let \(n,p_{n}\) be positive integers and \(\data := \set{\paren{y_{i}, \x_{i}}:i\in [n]}\) be a set of independent data points drawn from a distribution \(\prob_{y, \x}\) for \((y, \x)\in\R^{1+p_{n}}\). We treat the \(\x_{i}\) as predictors of the outcome \(y_{i}\), and we assume a linear model
\[\y = \X\bbeta + \e\]
where \(\X = {[\x_{1}\;\x_{2}\;\cdots\;\x_{n}]}^{\top}\in\R^{n\times p_{n}}\) is the design matrix, \(\y = \sqbr{y_{1}\; y_{2}\;\cdots\; y_{n}}^{\top}\), and \(\e\) is a mean-zero random vector with \(\Cov{\e} = \sigma_{2}\boldsymbol{I}_{n}\).

With the goal of model selection in mind, we let \(\Acal\subset2^{[p_{n}]}\) be a family of index sets representing candidate models. For \(\alpha\in\Acal\), we denote by \(p_{n}(\alpha)\) the cardinality of \(\alpha\) and consider the models given by
\[m_{\alpha}(\X) = \X_{\alpha}\bbeta_{\alpha},\]
where \(\X_{\alpha}\) is the sub-matrix of \(\X\) containing only the columns indexed by \(\alpha\), and \(\bbeta_{\alpha}\) is the coefficient vector containing only the entries indexed by \(\alpha\) in \(\bbeta\).
\begin{enumerate}
    \item We say \(\alpha\in\Acal\) is \emph{correct} if \(\Ep{\y\mid\X}\aseq m_{\alpha}(\X)\), and we denote by \(\Acal_{c}\) the set of correct models in \(\Acal\)
    \item We say \(\alpha\in\Acal\) is \emph{wrong} if it is not correct, and we denote by \(\Acal_{w}\) the set of wrong models in \(\Acal\)
    \item We say \(\Acal\) is \emph{embedded} if there exists an enumeration \(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{k}\) of all elements in \(\Acal\) such that \[\alpha_{1}\subset\alpha_{2}\subset\cdots\subset\alpha_{k}.\]
\end{enumerate}

\begin{definition}
    For \(\alpha\in\Acal\), let \(\bbetahat_{\alpha}\) be the OLS estimator of \(\bbeta_{\alpha}\) and \(\hat{m}_{\alpha}(\X):=\X_{\alpha}\bbetahat_{\alpha}\). We denote the average squared error of \(\hat{m}_{\alpha}\) by
    \[\Loss{\alpha}:=\frac{1}{n}\normsq{\Ep{\y\mid\X} - \hat{m}_{\alpha}\paren{\X}}.\]
    Additionally, we write
    \(R_{n}(\alpha):= \Ep{\Loss{\alpha}\mid\X}\).
\end{definition}

%\begin{tcolorbox}[title=Assumptions, fonttitle=\bfseries]
    The following conditions will be used throughout this section:
    \begin{align*}
        &\mathbf{H1:}\qquad \liminf_{n\to\infty} \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}>0 \text{ for all }\alpha \in\Acal.\\
        &\mathbf{H2:} \qquad \X^{\top}\X = O(n)\quad\text{and}\quad\paren{\X^{\top}\X}^{-1}= O\paren{n^{-1}}.\\[2.5mm]
        &\mathbf{H3:}\qquad \lim_{n\to\infty}\max_{i\leq n}\,h_{ii,\alpha} = 0 \text{ for all }\alpha \in\Acal.\\
        &\mathbf{H4:}\qquad \sum_{\alpha\in\Acal_{w}}\frac{1}{\paren{nR_{n}(\alpha)}^{m}}\to_{\prob}0\quad\text{for some }m\geq 1.
    \end{align*}
%\end{tcolorbox}

\begin{proposition}
    If we assume a linear model \(\y = \X\beta + \e\), then
    \[\Loss{\alpha} = \frac{1}{n}\normsq{H_{\alpha}\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\quad\text{and}\quad R_{n}(\alpha) = \frac{1}{n}\sigma^{2}p_{n}(\alpha) + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta},\]
    where \(H_{\alpha} = \X_{\alpha}\paren{\X_{\alpha}^{\top}\X_{\alpha}}^{-1}\X_{\alpha}^{\top}\) and \(M_{\alpha}= I_{n} - H_{\alpha}\).
\end{proposition}

\begin{proof}
    First, we have that
    \begin{align*}
        \normsq{\Ep{\y\mid\X} - \hat{m}_{\alpha}(\X)} &= \normsq{\X\bbeta - \X_{\alpha}\bbetahat_{\alpha}}\\
        &= \normsq{\X\bbeta - H_{\alpha}\paren{\X\bbeta + \e}}\\
        &= \normsq{M_{\alpha}\X\bbeta - H_{\alpha}\e}.
    \end{align*}
    Notice that \(M_{\alpha}\X\bbeta\) and \(H_{\alpha}\e\) are orthogonal:
    \[\e^{\top}H_{\alpha}M_{\alpha}\X\bbeta  = \e^{\top}H_{\alpha}\paren{I_{n} - H_{\alpha}}\X\bbeta = \e^{\top}H_{\alpha}\X\bbeta - \e^{\top}H_{\alpha}\X\bbeta = 0.\]
    Hence, the first part follows from the Pythagorean theorem.

    For the second part, we note that \(\Ep{\normsq{H_{\alpha}\e}\mid\X} = \sigma^{2}p_{n}(\alpha)\) by the ``trace trick'', where \(p_{n}(\alpha)\) denotes the size of model \(\alpha\).
\end{proof}

\begin{proposition}
    Suppose that the set of correct candidate models \(\Acal_{c}\subset\Acal\) is non-empty, and let \(\alpha^{*}\) be the smallest correct model in \(\Acal_{c}\). Then, \(\alpha^{*}\) minimizes \(R_{n}(\alpha)\) over \(\alpha\in\Acal\).
\end{proposition}

\begin{proof}
    Let \(\alpha\in\Acal\) be arbitrary and suppose that \(\alpha\in\Acal_{c}\). Then, \(\X_{\alpha}\bbeta_{\alpha} = \X\bbeta\) and \(p_{n}(\alpha^{*})\leq p_{n}(\alpha)\). Thus,
    \begin{align*}
        R_{n}(\alpha) &= \frac{1}{n}\sigma^{2}p_{n}(\alpha) + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\\[2mm]
        &= \frac{1}{n} \sigma^{2}p_{n}(\alpha) + \frac{1}{n}\underbrace{\normsq{M_{\alpha}\X_{\alpha}\bbeta_{\alpha}}}_{0}\\
        &= \frac{1}{n}\sigma^{2}p_{n}(\alpha)\; \geq \; \frac{1}{n}\sigma^{2}p_{n}(\alpha^{*}) = R_{n}(\alpha^{*}).
    \end{align*}
    Now suppose that \(\alpha\in\Acal_{w}\). If \(p_{n}(\alpha)\geq p_{n}(\alpha^{*})\), the result follows by assumption H1. If \(p_{n}(\alpha)\geq p_{n}(\alpha^{*})\), then \ldots \textcolor{red}{MISSING.}
\end{proof}

\subsection{Shao, 1993: Notes on Leave-One-Out CV}
In this section, we assume that \(p(\alpha):=p_n(\alpha)\) is constant for each \(\alpha\in\Acal\).
\begin{definition}
    The LOOCV estimator of \(R_{n}(\alpha)\) is definded as
    \[\loocv{\alpha}:= \frac{1}{n}\sum_{i=1}^{n}\paren{\frac{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}{1-h_{ii,\alpha}}}^{2}\]
\end{definition}

\begin{lemma}[Shao, 1993~\cite{shao_1993}]
    \[\loocv = \begin{cases}
        R_{n}(\alpha) + \sigma^{2} + \op{1} &\text{ if }\alpha\in\Acal_{w}\\[2mm]
        \frac{1}{n}\normsq{M_{\alpha}\e} + \frac{2}{n}\sigma^{2}p(\alpha) + \op{n^{-1}} &\text{ if }\alpha\in\Acal_{c}
    \end{cases}\]
\end{lemma}

\begin{proof}
    Using the Taylor expansion of \(1/{(1-x)}^{2} = 1+2x + O(x^{2})\), we have
    \[\frac{1}{\paren{1-\lev}^{2}} = 1 + 2\lev + \Op{\lev^{2}}.\]
    Thus,
    \begin{equation}
        \label{eq:xizeta}
        \loocv = \underbrace{\frac{1}{n}\sum_{i=1}^{n}\paren{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}^{2}}_{\xi_{\alpha,n}} + \underbrace{\frac{1}{n}\sum_{i=1}^{n}\paren{2\lev + \Op{\lev^{2}}}\paren{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}^{2}}_{\zeta_{\alpha,n}}
    \end{equation}
    Let \(\xi_{\alpha,n}\) and \(\zeta_{\alpha,n}\) denote the first and second terms in (\ref{eq:xizeta}), respectively. Note that
    \begin{align}
        \notag\xi_{\alpha,n} &= \frac{1}{n}\normsq{M_{\alpha}\X\bbeta + M_{\alpha}\e}\\
        \label{line:decomp}&= \frac{1}{n}\paren{\normsq{M_{\alpha}\e} + \normsq{M_{\alpha}\X\bbeta} + 2 \e^{\top}M_{\alpha}\X\bbeta}\\
        \notag&=\frac{1}{n}\normsq{\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + \frac{1}{n}\normsq{H_{\alpha}\e} + \frac{2}{n}\e^{\top}M_{\alpha}\X\bbeta\\
        \label{line:op1}&= \frac{1}{n}\normsq{\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + \op{1}.
    \end{align}
    The equality at (\ref{line:op1}) follows from the fact that \(\Ep{\normsq{H_{\alpha}\e}\mid\X} = \sigma^{2}p(\alpha)\) and 
    \[\Ep{\e^{\top}M_{\alpha}\X\bbeta\mid\X}^{2} = \sigma^{2}\normsq{M_{\alpha}\X\bbeta} = \Op{n},\qquad\textcolor{red}{(?)}\]
    so that \(1/n\normsq{H_{\alpha}\e}\to_{\prob}0\) and \(2/n\paren{\e^{\top}M_{\alpha}\X\bbeta}=\Op{1}\).\quad\textcolor{red}{(?)} % This is the reasoning given in the paper; I don't understand why the sum is \op(1).

    Since \(0<\lev<1\), \(2\lev + \Op{\lev^{2}}\leq\Op{\max_{i}\lev}\). Thus,
    \begin{equation}
        \label{eq:Opmax}
        \zeta_{\alpha,n}\leq\Op{\max_{i}\lev}\paren{\frac{1}{n}\sum_{i=1}^{n}\paren{y_{i} - \x_{i\alpha}^{\top}\bbetahat_{\alpha}}^{2}} = \Op{\max_{i}\lev}\xi_{\alpha,n}.
    \end{equation}
    (\ref{line:op1}) and (\ref{eq:Opmax}) imply the first case in the Lemma.\textcolor{red}{ DOES IT?}

    If \(\alpha\in\Acal^{c}\), it is easy to see from (\ref{line:decomp}) that \(\xi_{\alpha,n} = 1/n\normsq{M_{\alpha}\e}\), Furthermore,
    \[\zeta_{\alpha,n} = \frac{2}{n}\sigma^{2}p(\alpha) + \op{1},\qquad\textcolor{red}{(?)}\]
    proving the second case.
\end{proof}

\begin{proposition}[Shao, 1993~\cite{shao_1993}]
    Let \(\alphahat{(1)}\) be the model minimizing \(\loocv{\alpha}\).
    \begin{enumerate}
        \item Under H1, H2, and H3, \[\lim_{n\to\infty}\prob\paren{\alphahat{(1)}\in\Acal_{w}}=0.\]
        \item For \(\alpha\in\Acal_{c}\) with \(\alpha\neq\alpha^{*}\),\[\prob\paren{\loocv{\alpha}\leq \loocv{\alpha^{*}}} = \prob\paren{2\paren{p(\alpha) - p(\alpha^{*})}\sigma^{2} < \e^{\top}(H_{\alpha} - H_{\alpha^{*}})\e} + \op{1}.\] In particular, if \(\e\sim\Ncal(0_{n}, \sigma^{2}I_n)\), \[\prob\paren{\loocv{\alpha}\leq \loocv{\alpha^{*}}} = \prob\paren{2k < \chi^{2}(k)} + \op{1} \] for \(k=p(\alpha) - p(\alpha^{*})\).
        \item If \(p(\alpha^*) \neq p\), \[\lim_{n\to\infty}\prob\paren{\alphahat{(1)}=\alpha^{*}}\neq 1.\]
    \end{enumerate}
\end{proposition}

\begin{proof}
    \(\)

    \noindent1.\quad \textcolor{red}{MISSING}

    \noindent2.\quad The first part follows from Lemma 2.1 by algebraic manipulation. The second part follows by noting that, if \(\e\sim\Ncal(0_{n}, \sigma^{2}I_{n})\), then \[\frac{\e^{\top}}{\sigma}\paren{H_{\alpha} - H_{\alpha^{*}}}\frac{\e}{\sigma}\sim\chi^{2}\paren{\mathrm{tr}\paren{H_{\alpha}-H_{\alpha^{*}}}}.\]

    \noindent3.\quad If \(p(\alpha^{*}) = p\), then \(\Acal_{c} = \set{\alpha^{*}}\). It follows from 1.\ that \(\lim_{n\to\infty}\prob\paren{\alphahat{(1)}=\alpha^{*} }= 1\).
    Conversely, if \(\lim_{n\to\infty}\prob\paren{\alphahat{(1)}=\alpha^{*}} = 1\) \textcolor{red}{MISSING}
\end{proof}

\begin{corollary}
    LOOCV is not consistent. In particular, overfits with non-vanishing probability.
\end{corollary}

A subsequent result states that cross-validation is consistent if \(n_{v}/n\to 1\) as \(n\to\infty\), where \(n_{v}\) is the number of validation samples.

\subsection{Shao, 1997}
\begin{definition}
    Let \(\alphahat{}_{n}\) be the model selected by minimizing some criterion \(\hat{R}_{n}\) over \(\Acal\), and let \(\alpha^{*}_{n}\) denote the model minimizing \(R_{n}\) over \(\Acal\). We say \(\hat{R}_{n}\) is \emph{consistent} if 
    \[\prob\set{\alphahat{} = \alpha^{*}}\to 1\]
    as \(n\to\infty\). We say that \(\hat{R}_{n}\) is \emph{assomptotically loss efficient} if 
    \[\frac{R_{n}(\alphahat{})}{R_{n}(\alpha^{*}_{n})}\to 1\quad\text{a.s.}\]
\end{definition}

\begin{proposition}[Shao, 1997~\cite{shao_1997}]
    Suppose H1, \(p_{n}/n\to 0\), and that \(\Acal_{c}\) is non-empty for all but finitely many \(n\).
    \begin{enumerate}
        \item If \(|\Acal_{c}|=1\) for all but finitely many \(n\), then consistency is equivalent to efficiency in the sense of Definition 3.1
        \item If \(p_{n}(\alpha^{*}_{n})\not\to_{\prob}\infty\), then consistency is equivalent to efficiency in the sense of Definition 3.1
    \end{enumerate}
\end{proposition}
\begin{proof}
    \textcolor{red}{MISSING}
\end{proof}

\begin{definition}
    We define the GIC loss estimator to be
    \[\Rhat{\lambda_{n}}{\alpha} := \frac{\normsq{\y - \hat{m}(\X)}}{n} + \frac{1}{n}\lambda_{n}\sigmahat p_{n}(\alpha)\quad\text{for }\alpha\in\Acal,\]
    where \(\sigmahat\) is an estimator of \(\sigma^{2}\) and \(\lambda_{n}\) is a sequence of positive real numbers satisfying \(\lambda_{n}\geq 2\) and \(\lambda_{n}/n\to 0\).
\end{definition}

\subsubsection{The case of \(\lambda_{n} \equiv 2\)}

\begin{proposition}[Shao, 1997~\cite{shao_1997}]\label{prop:97prop1}
    Suppose that \(\lambda_{n}=2\) for all \(n\geq 1\) and that \(\sigmahat\) is a consistent estimator of \(\sigma^{2}\). Then,
    \[\Rhat{2}{\alpha} = \begin{cases}
        \textcolor{red}{TBD}
    \end{cases}\]
\end{proposition}

\begin{theorem}[Shao, 1997~\cite{shao_1997}]\label{prop:97thm1}
    Suppose that H4 holds and that \(\sigmahat\) is consistent for \(\sigma^{2}\). Then, \(\alphahat{2}_{n}\) is consistent and asymptotically loss efficient.
    \begin{enumerate}
        \item If \(|\Acal_{c}|\leq1\) for all but finitely many \(n\), then \(\alphahat{2}_{n}\) asymptotically loss efficient.
        \item Suppose that \(|\Acal_{c}|> 1\) for all but finitely many \(n\). If there exists a positive integer \(m\) such that \(\Ep{y_{1} - \x_{1}^{\top}\bbeta}^{4m}<\infty\) and 
        \begin{equation}
            \sum_{\alpha\in\Acal_{c}}\frac{1}{\paren{p_{n}(\alpha)}^{m}}\to 0 \quad\text{or}\quad \sum_{\substack{\alpha\in\Acal_{c},\\ \alpha\neq\alpha^{*}}}\frac{1}{\paren{p_{n}(\alpha) - p_{n}(\alpha^{*})}^{m}},
        \end{equation}
        then \(\alphahat{2}_{n}\) is assymptotically loss efficient.
        \item Suppose that \(|\Acal_{c}>1|\) for all but finitely many \(n\) and that for any integer \(q\) and constant \(c>2\), 
        \begin{equation}
            \label{eq:conditionQ}
            \liminf_{n\to\infty}\inf_{Q_{n}\in\mathcal{Q}_{n,q}}\prob\set{\e_{n}^{\top}Q_{n}\e_{n} > c\sigma^{2}q} > 0,
        \end{equation}
        where \(\mathcal{Q}_{n,q}\) is the set of all projection matrices of rank \(q\). The condition that
        \begin{equation}
            \label{eq:conditionP}
            p_{n}(\alpha^{*}_{n})\to\infty\quad\text{or}\quad \min_{\substack{\alpha\in\Acal_{c},\\ \alpha\neq\alpha^{*}}}\paren{p_{n}(\alpha) - p_{n}(\alpha^{*})}\to\infty
        \end{equation}
        is necessary and sufficient for the asymptotic loss efficiency of \(\alphahat{2}_{n}\) whenever \(|\Acal_{c}|\) is bounded or \(\Acal\) is embedded.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \textcolor{red}{MISSING}
\end{proof}

Note that condition (\ref{eq:conditionQ}) is satisfied if \(\e\sim\Ncal(0_{n}, \sigma^{2}I_{n})\). Condition (\ref{eq:conditionP}) is satisfied if \(\Acal\) does not contain two correct models with fixed dimensions for all but finitely many \(n\).
\begin{corollary}[Shao, 1997]
    If \(\Acal_{c}\) contains exactly one model with fixed dimension for all but finitely many \(n\), then \(\alphahat{2}_{n}\) is consistent.
\end{corollary}

\begin{proof}
    This follows immediately from Theorem~\ref{prop:97thm1} and Proposition~\ref{prop:97prop1}.
\end{proof}

\textcolor{red}{INCOMPLETE:\@ Missing \(\lambda_{n}\to\infty\) and cross-validation discussion.}

\section{CV for Nonparametric Model Selection}

\subsection{Yang, 2007}

Here we consider two regression procedures, denoted \(\delta_{1}\) and \(\delta_{2}\), that yield estimators \(\hat{f}_{n, 1}\) and \(\hat{f}_{n, 2}\) of the regression function stisfying
\begin{equation}\label{eq:regressionmodel}
    y_{i} = f(\x_{i}) + \epsilon_{i}\quad i\in[n],
\end{equation}
for \(\x_{i}\) iid, \(\Ep{\epsilon_{i}\mid \X} \aseq 0\) and \(\Ep{\epsilon_{i}^{2}\mid \X} \stackrel{\text{a.s.}}{<} \infty\).

\begin{definition}
    We say \(\delta_{1}\) is \emph{assymptotically better} than \(\delta_{2}\) under the loss function \(L\) if, for \(0<\varepsilon<1\), there exists \(c_{\varepsilon}>0\) such that
    \[\prob\set{\Loss{\fhat{n}{2}}\geq (1+c_{\epsilon})\Loss{\fhat{n}{2}} }\geq 1-\varepsilon.\]

    Given that \(\delta_{1}\) is assymptotically better than \(\delta_{2}\), we say that a selection procedure is consistent if it selects \(\delta_{1}\) with probability tending to 1 as \(n\to\infty\).
\end{definition}


\subsubsection{Single-split cross-validation (the Hold-out)}

For this section, we assume that the first \(n_1\) elements in \(\data\) are used as a training/estimation sample and the remaining \(n_2\) elements make up the validation sample. We write \(p_{n}\) and \(q_{n}\) for the rates of convergence of the estimators \(\fhat{n}{1}\) and \(\fhat{n}{2}\), respectively. That is,
\[\Op{p_{n}} = \norm{f - \fhat{n}{1}}_{2}\quad\text{and}\quad \Op{q_{n}} = \norm{f- \fhat{n}{2}}_{2}.\]

The hold-out cross-validation method consists in selecting the estimator that minimizes the hold-out loss
\[L_{\mathrm{ho}}(\fhat{n}{j}) = \sum_{i=n_{1}+1}^{n}\paren{y_{i} - \fhat{n}{j}(\x_{i})}^{2}\quad\text{for }j=1,2.\]
The propositions in this section rely on the following conditions:
\begin{itemize}
    \item \textbf{A2.1:} \(\Ep{\epsilon_{i}^{2}\mid \x_{i}}\) is bounded a.s.\ for \(i\in[n]\).
    \item \textbf{A2.2:} There exists \(A_{n}\) such that \(\norm{f-\fhat{n}{j}}_{\infty} = \Op{A_{n}}\) for \(j=1,2\).
    \item \textbf{A2.3:} One procedure is asymptotically better than the other.
    \item \textbf{A2.4:} There exists \(M_{n}\) such that \(\norm{f-\fhat{n}{j}}_{4} / \norm{f-\fhat{n}{j}}_{4} = \op{M_{n}}\) for \(j=1,2\).
\end{itemize}

\begin{theorem}[Yang, 2007~\cite{yang_2007}]\label{prop:yangth1}
    Suppose that \textbf{A2.1}--\textbf{A2.4} hold. Suppose, furthermore, that
    \begin{enumerate}
        \item \(n_{1}\to\infty\)
        \item \(n_{2}\to\infty\)
        \item \(n_{2}M_{n}^{-4} \to \infty\)
        \item \(\sqrt{n_{2}}\max(p_{n_{1}}, q_{n_{1}})\)
    \end{enumerate}
    Then, the hold-out CV procedure is consistent.
\end{theorem}

A very detailed proof of this result is provided in Yang~\cite{yang_2007}, so it will be skipped here.

\subsubsection{Voting cross-validation with multiple splits}

The (theoretical) majority-vote cross-validation method proceeds as follows: for each permutation \(i\mapsto\pi(i)\) of the data, we compute the estimators \(\fhat{n_{1}}{1}\) and \(\fhat{n_{1}}{2}\) using the first \(n_{1}\) data points \((y_{\pi(1)}, \x_{\pi(1)}), \ldots,(y_{\pi(n_1)}, \x_{\pi(n_1)})\) as the training sample and the remaining \(n_{2}=n-n_{1}\) elements as the validation sample. We then find the estimator that minimizes the hold-out loss
\[L_{\pi}(\fhat{n_{1}}{j}) = \sum_{i=n_{1}+1}^{n}\paren{y_{\pi(i)} - \fhat{n_{1}}{j}\paren{\x_{\pi(i)}}}^{2}\qquad \text{for }j=1,2.\]
The chosen estimator is the one favored by the majority of the permutations. More formally, we define 
\[\tau_{\pi} = \ind{L_{\pi}(\fhat{n_{1}}{1}) \leq L_{\pi}(\fhat{n_{1}}{2})}\]
We then define our selection criterion as follows:
\[\hat{f}_{n} = \begin{cases}
    \fhat{n}{1} &\text{if }\sum_{\pi\in\Pi}\tau_{\pi} \geq {n!}/{2},\\[2mm]
    \fhat{n}{2} &\text{otherwise,}
\end{cases}\]
where \(\Pi\) denotes the set of all permutations of \([n]\).

\begin{theorem}[Yang, 2007~\cite{yang_2007}]\label{prop:yangth2}
    Under the conditions of Theorem 4.1 and the condition that the data is iid, the majority-vote cross-validation method is consistent.
\end{theorem}

\begin{proof}
    Suppose that \(\delta_{1}\) is asymptotically better than \(\delta_{2}\). For \(\pi\in \Pi\), we have that
    \[\prob\set{L_{\pi}\paren{\fhat{n_{1}}{1}} \leq L_{\pi}\paren{\fhat{n_{1}}{2}}} = \Ep{\tau_{\pi}} \stackrel{(*)}{=} \Ep{\frac{1}{n!}\sum_{\pi\in\Pi}\tau_{\pi}}.\]
    The equality at \((*)\) follows from the fact that the data are iid, hence exchangeable, and thus the \(\tau_\pi\) are identically distributed. By Theorem~\ref{prop:yangth1}, the right-hand side converges to 1 as \(n\to\infty\). Since the average \(1/n! \sum_{\pi}\tau_{\pi}\) is almost surely at most 1, it follows that \(1/n! \sum_{\pi}\tau_{\pi} \to 1\) in probability, and the majority-vote cross-validation method is consistent.
\end{proof}

The proof of Theorem~\ref{prop:yangth2} does not require using the entire set \(\Pi\) of permutations for the majority vote. In fact, Theorem~\ref{prop:yangth1} establishes that even a single data split suffices for consistency, provided the splitting conditions are met. Moreover, Yang~\cite{yang_2007} presents a counterexample demonstrating that these conditions are not merely sufficient but necessary, hence showing that the number of splits does not affect consistency. In other words, multiple splits in cross-validation cannot rescue an inconsistent single-split procedure. A natural question, then, is: if multiple splits do not improve consistency, what is their benefit? This will be explored in the simulation later on.~\textcolor{red}{(MAYBE?)}

\section{Aggregation}
\subsection{Bunea et al., 2007}

As before, we consider independent pairs in \(\data := \set{\paren{y_{i}, \x_{i}}:i\in [n]}\) satisfying (\ref{eq:regressionmodel}). Suppose that we have \(M\) candidate estimators of the regression function, denoted \(\fhat{n}{1}, \fhat{n}{2}, \ldots, \fhat{n}{M}\). Instead of selecting a single estimator, we combine them into an \emph{aggregate} \(\ftilde{\lambdahat{}}\) given by
\[\ftilde = \sum_{j=1}^{M}\lambdahat{j} \fhat{n}{j},\]
with \(\lambdahat{}:=\paren{\lambdahat{1}, \ldots, \lambdahat{M}}\in \Lambda\subset\R^{M}\) chosen to satisfy some optimality criterion.

\subsubsection{Four types of aggregation}

There are four aggregation schemes considered in Bunea et al.~\cite{bunea_2007}, each of which is characterized by a different set \(\Lambda\) of admissible weights \(\lambdahat{}\):
\begin{itemize}
    \item Model Selection Aggregation (MS): A single estimator is selected. That is,\[\Lambda_{\mathrm{MS}} = \set{\lambda\in\R^{M}:\lambda = \boldsymbol{e}_{j}\text{ for some }j\in[M]}.\]
    \item Linear Aggregation (L): \(\ftilde{\lambdahat{}}\) is chosen among all linear combinations of the estimators. That is, \[\Lambda_{\mathrm{L}} = \R^{M}.\]
    \item Convex Aggregation (C): \(\ftilde{\lambdahat{}}\) is chosen among all convex combinations of the estimators. That is, \[\Lambda_{\mathrm{C}} = \set{\lambda\in\R^{M}:\lambda\geq 0, \sum_{j=1}^{M}\lambda_{j} = 1}.\]
    \item Subset Selection (S): We select and aggregate at most \(D\) estimators from the pool, for a given \(D\leq M\). That is, \[\Lambda_{\mathrm{S}} = \set{\lambda\in\R^{M}:\lambda\text{ has at most \(D\) non-zero entries}}.\]
\end{itemize}

\subsubsection{Optimality criteria}

In an ideal scenario, we would like to select  weights \(\lambda^*\) satisying 
\[\lambda^{*} = \argmin_{\lambda\in\Lambda}\Ep{d\paren{f, \ftilde{\lambda}}}\]
for some distance function \(d\) (e.g., the \(L_{2}\) norm). However, since the true regression function \(f\) is unknown, this approach is clearly not feasible. Another way of constructing an estimator is to minimize its maximum risk on a class of functions \(\Theta\) containing \(f\). That is, we would like to find \(\lambdahat{}\) satisfying 
\[\sup_{f\in\Theta}\E\normsq{f-\ftilde{\lambdahat{}}}_{2} = \inf_{\lambda\in\Lambda}\sup_{f\in\Theta}\E\normsq{f-\ftilde{\lambda}}_{2}.\]
This is known as \emph{minimax} extimation. 
\marginpar{\footnotesize{not sure about this}}
\textcolor{red}{However, once again, this is not an easy task, as \(\Theta\) may be too large or unknown. We also know that the expected risk goes to 0 as \(n\to \infty\).}

Instead, we consider an alternative approach using \emph{oracles.} 

\begin{definition}[adapted from Tsybakov, 2009~\cite{tsybakov_introduction_2009}]
    Suppose that there exists \(\lambda^{*}\in\Lambda\) such that 
    \[\E\normsq{f-\ftilde{\lambda^{*}}}_{2} = \inf_{\lambda\in\Lambda}\E\normsq{f-\ftilde{\lambda^{*}}}_{2}.\]
    The function \(f\mapsto \ftilde{\lambda^{*}}\) is called the oracle of aggregation under \(L_{2}\).

    We say that the aggregate \(\ftilde{\lambdahat{}}\) {mimics} the oracle if
    \begin{equation}\label{eq:oracleineq}
        \E\norm{f - \ftilde{\lambdahat{}}}_{2} \leq \inf_{\lambda\in\Lambda}\E\norm{f - \ftilde{\lambda}}_{2} + \Delta_{n,M}.
    \end{equation}
    for the smalles possible \(\Delta_{n,M}>0\) independent of \(f\).
\end{definition}

In what follows, the goal is to find lower bounds on \(\Delta_{n,M}\) for each of the aggregation schemes. We adapt Theorem 5.1 in~\cite{bunea_2007} to consider exclusively the \(L_{2}\) norm:

\begin{definition}[Tsybakov, 2009~\cite{tsybakov_introduction_2009}]
    For a class of functions \(\Theta\), a sequence \(\set{\psi_{n}}_{n\geq1}\) of positive numbers is called an \emph{optimal rate of convergence} of estimators \(\hat{f}\) on \(\Theta\) under \(L_{2}\) if there exist constants \(c, C>0\) such that
    \begin{align}
        &\limsup_{n\to\infty}\paren{\psi_{n}^{-2}\inf_{\hat{f}}\sup_{f\in\Theta}\Ep{\norm{f - \hat{f}}_{2}^{2}}}\leq C\\
        \text{and}\qquad&\liminf_{n\to\infty}\paren{\psi_{n}^{-2}\inf_{\hat{f}}\sup_{f\in\Theta}\Ep{\norm{f - \hat{f}}_{2}^{2}}}\geq c
    \end{align}
    \textcolor{red}{An estimator \(\hat{f}_{n}\) is said to be \emph{rate-optimal} if 
    \[\sup_{f\in\Theta}\Ep{\norm{f - \hat{f}_{n}}_{2}^{2}} \leq C'\psi_{n}^{2}\]
    for some \(C'>0\). It is called \emph{asymptotically efficient} of \(\Theta\) under \(L_{2}\) if 
    \[\lim_{n\to\infty}\frac{\sup_{f\in\Theta}\E{\normsq{f-\hat{f}_{n}}}}{\inf_{\hat{f}}\sup_{f\in\Theta}\E{\norm{f - \hat{f}}_{2}^{2}}}=1.\]}
\end{definition}

%\begin{itemize}
%    \item \textbf{A3.1:} The residuals \(\epsilon_i\) are iid \(\Ncal(0, \sigma^{2})\) for \(0<\sigma^2<\infty\)
%    \item \textbf{A3.2:} There exists \(A_{n}\) such that \(\norm{f-\fhat{n}{j}}_{\infty} = \Op{A_{n}}\) for \(j=1,2\).
%    \item \textbf{A3.3:} One procedure is asymptotically better than the other.
%    \item \textbf{A3.4:} There exists \(M_{n}\) such that \(\norm{f-\fhat{n}{j}}_{4} / \norm{f-\fhat{n}{j}}_{4} = \op{M_{n}}\) for \(j=1,2\).
%\end{itemize}

\begin{theorem}[Bunea et al., 2007~\cite{bunea_2007}]\label{prop:buneath5.1}
    \textcolor{red}{(Statement of lower bounds)}
    \[\sup_{f_1, \ldots, f_2\in\mathcal{F}_{0}} \inf_{T_n}\sup_{f\in\mathcal{F}_0} \set{\E\normsq{f - T_n}_{2} - \min_{\lambda\in\Lambda}\normsq{f - \ftilde{\lambda}}_{2}} \geq c\psi_{n}\]
\end{theorem}

\newpage
\printbibliography{}
\end{document}