\documentclass[12pt, letter paper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,geometry,fancyhdr,graphicx,xcolor,biblatex}
\usepackage{enumerate}
%\usepackage{minted}
%\usepackage[most]{tcolorbox}

\addbibresource{citations.bib}
\geometry{letterpaper, margin=1in}

% For indicator functions:
\DeclareMathAlphabet{\mathmybb}{U}{bbold}{m}{n}
\newcommand{\1}{\mathmybb{1}}

%\newtcolorbox[auto counter, number within=section]{proposition}[2][Proposition]{%
%  colframe=blue!50!black,
%  colback=blue!10,
%  coltitle=white,
%  fonttitle=\bfseries,
%  title=#1~\thetcbcounter\ifx#2\empty\else~(#2)\fi}
%
%\newtcolorbox[auto counter, number within=section]{definition}{%
%  fonttitle=\bfseries,
%  title=Definition~\thetcbcounter,
%  label={def:\thetcbcounter}}


\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Borel}[1]{\mathcal{B}\paren{#1}}
\newcommand{\0}{\emptyset}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ep}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\paren}[1]{\left(#1 \right)}
\newcommand{\sqbr}[1]{\left[#1 \right]}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\norm}[1]{|\hspace{-1pt}|#1 |\hspace{-1pt}|}
\newcommand{\normsq}[1]{\norm{#1}^{2}}
\newcommand{\ind}[1]{\mathmybb{1}_{\sqbr{#1}}}
\newcommand{\data}{\mathcal{D}_{n}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Acal}{\mathcal{A}_{n}}
\newcommand{\Tcal}{\mathcal{T}_{n}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\aseq}{\stackrel{\mathrm{a.s.}}{=}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\Cov}[1]{\mathrm{Cov}\paren{#1}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\Loss}[1]{L_{n}\paren{#1}}
\newcommand{\Rhat}[2]{\hat{R}_{n, #1}\paren{#2}}
\newcommand{\alphahat}[1]{\hat{\alpha}^{#1}}
\newcommand{\alphatilde}{\tilde{\alpha}}
\newcommand{\alphabar}{\bar{\alpha}}
\newcommand{\lev}{h_{ii,\alpha}}
\newcommand{\loocv}[1]{\hat{R}^{(1)}_{n}\paren{#1}}
\newcommand{\op}[1]{o_{\prob}\paren{#1}}
\newcommand{\Op}[1]{O_{\prob}\paren{#1}}
\newcommand{\sigmahat}{\hat{\sigma}^{2}_{n}}
\newcommand{\fhat}[2]{\hat{f}_{#1, #2}}
\newcommand{\ftilde}[1]{\tilde{f}_{#1}}
\newcommand{\lambdahat}[1]{\hat{\lambda}_{#1}}
\newcommand{\blambdahat}{\boldsymbol{\hat{\lambda}}}
\newcommand{\penBIC}[1]{\mathrm{pen}_{\mathrm{BIC}}\paren{#1}}

\title{Project: Cross-validation for model selection

(notes on papers)}
\author{Diego Urdapilleta de la Parra}
\begin{document}

\maketitle
\tableofcontents

\newpage
\section{Introduction}

The goal of this project is to study the asymptotic properties of cross-validation (CV) methods for model selection in a variety of scenarios.~\textcolor{red}{[(\ldots) motivation, brief description of CV methods, outline of the project.]}

Throughout the paper, we consider the usual regression setup: Let \(n,p_{n}\) be positive integers and \(\data := \set{\paren{y_{i}, \x_{i}}:i\in [n]}\) be a set of independent data points drawn from a distribution \(\prob_{y, \x}\) for \((y, \x)\in\R\times \mathcal{X}\). We treat the \(\x_{i}\)'s as predictors of the outcome \(y_{i}\), and we assume a model
\begin{equation}\label{eq:setup}
    y_{i} = f(\x_{i}) + e_{i},\qquad i\in[n],
\end{equation}
where \(f\) is an unknown Borel-measurable function \(f:\mathcal{X}\to\R\) with \(f(x_i)\aseq \Ep{y_i\mid \x_i}\) and the \(e_{i}\)'s are zero-mean random variables.



\section{CV for Linear Model Selection}
\subsection{Setup and preliminary results}
In this section, the regression function \(f\) in (\ref{eq:setup}) is assumed to be linear, so that the data is generated from a linear model of the form
\[\y = \X\bbeta + \e\]
where \(\X = {[\x_{1}\;\x_{2}\;\cdots\;\x_{n}]}^{\top}\in\R^{n\times p_{n}}\) is the design matrix, \(\y = \sqbr{y_{1}\; y_{2}\;\cdots\; y_{n}}^{\top}\), and \(\e\) is a mean-zero random vector with \(\Cov{\e} = \sigma_{2}\boldsymbol{I}_{n}\).

In the context of linear models, the model selection procesure reduces to selecting a subset of covariates from a set of candidate covariates of size \(p_n\). This is also known as \emph{variable selection}. We remark that the number \(p_n\) may depend on \(n\), and some assumptions on the growth of \(p_n\) will be established later.

We let \(\Acal\subset2^{[p_{n}]}\) be a family of index sets representing candidate models. For \(\alpha\in\Acal\), we denote by \(p_{n}(\alpha)\) the cardinality of \(\alpha\) and consider the model given by
\[f_{\alpha}(\X) = \X_{\alpha}\bbeta_{\alpha},\]
where \(\X_{\alpha}\) is the sub-matrix of \(\X\) containing only the columns indexed by \(\alpha\), and \(\bbeta_{\alpha}\) is the coefficient vector containing only the entries indexed by \(\alpha\) in \(\bbeta\).
\begin{enumerate}
    \item We say \(\alpha\in\Acal\) is \emph{correct} if \(\Ep{\y\mid\X}\aseq f_{\alpha}(\X)\), and we denote by \(\Tcal\) the set of correct models in \(\Acal\)
    \item We say \(\alpha\in\Acal\) is \emph{wrong} if it is not correct, and we denote by \(\Tcal^{c}\) the set of wrong models in \(\Acal\)
    \item We say \(\Acal\) is \emph{embedded} if there exists an enumeration \(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{k}\) of all elements in \(\Acal\) such that \[\alpha_{1}\subset\alpha_{2}\subset\cdots\subset\alpha_{k}.\]
\end{enumerate}

\begin{definition}\label{def:loss}
    For \(\alpha\in\Acal\), let \(\bbetahat_{\alpha}\) be the OLS estimator of \(\bbeta_{\alpha}\) and \(\hat{f}_{\alpha}(\X):=\X_{\alpha}\bbetahat_{\alpha}\). We denote the average squared error of \(\hat{f}_{\alpha}\) by
    \[\Loss{\alpha}:=\frac{1}{n}\normsq{f(\X) - \hat{f}_{\alpha}\paren{\X}}.\]
    Additionally, we write
    \[R_{n}(\alpha):= \Ep{\Loss{\alpha}\mid\X}.\]
\end{definition}

The following conditions will be used throughout out treatment of linear models:
\begin{align*}
    &\mathbf{H1:}\qquad \liminf_{n\to\infty} \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}>0 \text{ for all }\alpha \in\Acal.\\
    &\mathbf{H2:} \qquad \X^{\top}\X = O(n)\quad\text{and}\quad\paren{\X^{\top}\X}^{-1}= O\paren{n^{-1}}.\\[2.5mm]
    &\mathbf{H3:}\qquad \lim_{n\to\infty}\max_{i\leq n}\,h_{ii,\alpha} = 0 \text{ for all }\alpha \in\Acal.\\
    &\mathbf{H4:}\qquad \sum_{\alpha\in\Tcal^{c}}\frac{1}{\paren{nR_{n}(\alpha)}^{m}}\to_{\prob}0\quad\text{for some }m\geq 1.
\end{align*}

\begin{proposition}
    Assumimg a linear model \(\y = \X\beta + \e\),
    \[\Loss{\alpha} = \frac{1}{n}\normsq{H_{\alpha}\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\quad\text{and}\quad R_{n}(\alpha) = \frac{1}{n}\sigma^{2}p_{n}(\alpha) + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\]
    almost surely, where \(H_{\alpha} = \X_{\alpha}\paren{\X_{\alpha}^{\top}\X_{\alpha}}^{-1}\X_{\alpha}^{\top}\) and \(M_{\alpha}= I_{n} - H_{\alpha}\).
\end{proposition}

\begin{proof}
    First, we have that
    \begin{align*}
        \normsq{f(\X) - \hat{f}_{\alpha}(\X)} &= \normsq{\X\bbeta - \X_{\alpha}\bbetahat_{\alpha}}\\
        &= \normsq{\X\bbeta - H_{\alpha}\paren{\X\bbeta + \e}}\\
        &= \normsq{M_{\alpha}\X\bbeta - H_{\alpha}\e}.
    \end{align*}
    Notice that \(M_{\alpha}\X\bbeta\) and \(H_{\alpha}\e\) are orthogonal:
    \[\e^{\top}H_{\alpha}M_{\alpha}\X\bbeta  = \e^{\top}H_{\alpha}\paren{I_{n} - H_{\alpha}}\X\bbeta = \e^{\top}H_{\alpha}\X\bbeta - \e^{\top}H_{\alpha}\X\bbeta = 0.\]
    Hence, the first part follows from the Pythagorean theorem.

    For the second part, we note that \(\Ep{\normsq{H_{\alpha}\e}\mid\X} \aseq \sigma^{2}p_{n}(\alpha)\) by the ``trace trick'', where \(p_{n}(\alpha)\) denotes the size of model \(\alpha\).
\end{proof}

\begin{proposition}\label{prop:Ropt}
    Suppose that that \(\Tcal\) is non-empty, and let \(\alpha^{*}_{n}\) be the smallest correct model in \(\Tcal\). Then, \(\alpha^{*}_{n}\) minimizes \(R_{n}(\alpha)\) \textcolor{red}{(with probability 1?)} over \(\alpha\in\Acal\).
\end{proposition}

\begin{proof}
    Let \(\alpha\in\Acal\) be arbitrary and suppose that \(\alpha\in\Tcal\). Then, \(\X_{\alpha}\bbeta_{\alpha} = \X\bbeta\) and \(p_{n}(\alpha_{n}^{*})\leq p_{n}(\alpha)\). Thus,
    \begin{align*}
        R_{n}(\alpha) &= \frac{1}{n}\sigma^{2}p_{n}(\alpha) + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\\[2mm]
        &= \frac{1}{n} \sigma^{2}p_{n}(\alpha) + \frac{1}{n}\underbrace{\normsq{M_{\alpha}\X_{\alpha}\bbeta_{\alpha}}}_{0}\\
        &= \frac{1}{n}\sigma^{2}p_{n}(\alpha)\; \geq \; \frac{1}{n}\sigma^{2}p_{n}(\alpha_{n}^{*}) = R_{n}(\alpha_{n}^{*}).
    \end{align*}
    Now suppose that \(\alpha\in\Tcal^{c}\). If \(p_{n}(\alpha)\geq p_{n}(\alpha_{n}^{*})\), the result follows immediately by assumption H1. On the other hand, if \(p_{n}(\alpha)\leq p_{n}(\alpha_{n}^{*})\), we must verify that
    \begin{equation}\label{eq:Roptgoal}
        \normsq{M_{\alpha}\X\bbeta} \geq \sigma^{2}\paren{p_{n}(\alpha_{n}^{*}) - p_{n}(\alpha)}.
    \end{equation}
    To this end, we note that \(\normsq{M_{\alpha}\X\bbeta} = \normsq{M_{\alpha}\X_{\alpha_{n}^{*}}\bbeta_{\alpha_{n}^{*}}}\) and that 
    \[\X_{\alpha_{n}^{*}}\bbeta_{\alpha_{n}^{*}} = \X_{\alpha}\bbeta_{\alpha} + \X_{\alpha_{n}^{*}\setminus\alpha}\bbeta_{\alpha_{n}^{*}\setminus\alpha} .\]
    Thus, if we let \(\lambda\) denote the smallest eigenvalue of \(\X_{\alpha_{n}^{*}}^{\top}M_{\alpha}\X_{\alpha_{n}^{*}}\), we have that
    \[\normsq{M_{\alpha}\X\bbeta} = \normsq{M_{\alpha}\X_{\alpha_{n}^{*}\setminus\alpha}\bbeta_{\alpha_{n}^{*}\setminus\alpha}} \geq \lambda\normsq{\bbeta_{\alpha*\setminus\alpha}}\]
    (for a proof of the latter inequality, see~\cite{hansen_2022}).
    \textcolor{red}{This is as far as I got. I don't know how to show that \(\lambda\normsq{\bbeta_{\alpha_{n}^{*}\setminus\alpha}}\geq \sigma^{2}\paren{p_{n}(\alpha_{n}^{*}) - p_{n}(\alpha)}\), but it seems reasonable if the coefficients in \(\bbeta\) are not too small.}
\end{proof}

From Proposition~\ref{prop:Ropt}, we see that \(R_{n}\) is a good choice of selection criterion. Unfortunately, \(R_{n}\) depends on the unkown regression function, and therefore cannot be used in practice. Instead, we may try to approximate it through other empirically feasible criteria.

\begin{definition}\label{def:consistency97}
    Let \(\alphahat{}_{n}\) be the model selected by minimizing some criterion \(\hat{R}_{n}\) over \(\Acal\), and let \(\alpha^{*}_{n}\) denote the model minimizing \(R_{n}\) over \(\Acal\). We say \(\hat{R}_{n}\) is {consistent} if 
    \[\prob\set{\alphahat{}_{n} = \alpha^{*}_{n}}\to 1\]
    as \(n\to\infty\). We say that \(\hat{R}_{n}\) is {assomptotically loss efficient} if 
    \[\frac{L_{n}(\alphahat{}_{n})}{L_{n}(\alpha^{*}_{n})}\xrightarrow{\prob} 1.\]
\end{definition}

\begin{lemma}\label{lemm:implication}
    If \(\hat{R}_{n}\) is consistent, then it is asymptotically loss efficient.
\end{lemma}

\begin{proof}
    Suppose that \(\hat{R}_{n}\) is consistent. Clearly, if \(\alphahat{}_{n} = \alpha^{*}_{n}\), then \(\Loss{\alphahat{}} = \Loss{\alpha^{*}_{n}}\). Therefore, 
    \[\prob\set{\alphahat{}_{n} = \alpha^{*}_{n}}\leq \prob\set{\Loss{\alphahat{}} = \Loss{\alpha^{*}_{n}}}.\]
    By consistency, the left-hand side converges to 1, so that the right-hand side must also converge to 1.
\end{proof}

\begin{proposition}[Shao, 1997~\cite{shao_1997}]
    Suppose H1, \(p_{n}/n\to 0\), and that \(\Tcal\) is non-empty for all but finitely many \(n\).
    \begin{enumerate}
        \item If \(|\Tcal|=1\) for all but finitely many \(n\), then consistency is equivalent to efficiency in the sense of Definition~\ref{def:consistency97}.
        \item If \(p_{n}(\alpha^{*}_{n})\not\xrightarrow{\prob}\infty\), then consistency is equivalent to efficiency in the sense of Definition~\ref{def:consistency97}.
    \end{enumerate}
\end{proposition}
\begin{proof}
    From Lemma~\ref{lemm:implication}, it remains to show that, under the given conditions, assymptotic loss efficiency implies consistency. We show the contrapositive:

    \noindent1.\quad Suppose that \(\hat{R}_{n}\) is not consistent. By Proposition~\ref{prop:Ropt}, \(\alpha^{*}_{n}\) must be the correct model in \(\Tcal\) minimizing \(R_{n}\). Therefore, \(\Loss{\alpha^{*}_{n}} = (1/n)\normsq{H_{\alpha}\e}\), and
    \[\Ep{\Loss{\alpha^{*}_{n}}} = \frac{1}{n}\sigma^{2}p_{n}(\alpha^{*}_{n})\leq \frac{1}{n}\sigma^{2}p_{n}\to 0 \quad\text{as }n\to\infty\]
    by assumption. We have shown that \(\Loss{\alpha^{*}_{n}}\xrightarrow{L_{1}} 0\), which implies \(\Loss{\alpha^{*}_{n}}\xrightarrow{\prob} 0\).

    On the other hand, since \(\hat{R}_{n}\) is not consistent, there must exist \(\alphatilde_{n}\neq\alpha^{*}\) for infinitely many \(n\) such that \(\prob\set{\alphahat{}_{n} = \alphatilde_{n}} \neq 0\). Notice that, since \(\Tcal=\set{\alpha^{*}}\), it must be the case that \(\alphatilde_{n}\in\Tcal^{c}\). We have the following:
    \[\Loss{\alphahat{}_{n}}\geq \ind{\alphahat{}_{n} = \alphatilde_{n}}\Loss{\alphatilde_{n}} = \ind{\alphahat{}_{n} = \alphatilde_{n}}\paren{\frac{1}{n}\normsq{H_{\alphatilde_{n}}\e + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}}}.\]
    By assumption \textbf{H1}, the latter expression cannot not converge to 0. We conclude that the ratio \(\Loss{\alphahat{}_{n}}/\Loss{\alpha^{*}}\not\xrightarrow{\prob} 1\).

    \noindent2.\quad Suppose again that \(\hat{R}_{n}\) is not consistent. Since \(\Tcal\) contains at least two models, there must exist \(\alphatilde_{n}\in\Tcal\) such that \(\alphatilde_{n}\neq \alpha^{*}\) and \(\prob\set{\alphahat{}_{n} = \alphatilde_{n}}\not\to 0\). Hence,
    \[\frac{\Loss{\alphahat{}_{n}}}{\Loss{\alpha^{*}_{n}}} - 1 \geq \paren{\frac{\Loss{\alphatilde{}_{n}}}{\Loss{\alpha^{*}_{n}}} - 1}\ind{\alphahat{}_{n} = \alphatilde_{n}} = \paren{\frac{\normsq{H_{\alphatilde_{n}}}}{\normsq{H_{\alpha^{*}_{n}}}} - 1}\ind{\alphahat{}_{n} = \alphatilde_{n}}\not\xrightarrow{\prob}0.\]
\end{proof}

\subsection{A Result on the Leave-one-out: Shao, 1993}
\renewcommand{\Acal}{\mathcal{A}}
\renewcommand{\Tcal}{\mathcal{T}}
\textcolor{red}{[Brief introduction to LOOCV and motivation]}

For this section, we will consider the case where the set \(\Acal_{n}=:\Acal\) and all its elemets are constant across all \(n\geq 1\). That is, the candidate models are not changed by the number of observations.
\begin{definition}
    The Leave-one-out estimator of \(R_{n}(\alpha)\) is definded as
    \[\loocv{\alpha}:= \frac{1}{n}\sum_{i=1}^{n}\paren{\frac{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}{1-h_{ii,\alpha}}}^{2}\]
\end{definition}

\begin{lemma}[Shao, 1993~\cite{shao_1993}]\label{lem:lemmadecomp}
    \begin{equation}\label{eq:lemmadecomp}
    \loocv{\alpha} = \begin{cases}
        R_{n}(\alpha) + \sigma^{2} + \op{1} &\text{ if }\alpha\in\Tcal^{c}\\[2mm]
        \frac{1}{n}\normsq{M_{\alpha}\e} + \frac{2}{n}\sigma^{2}p(\alpha) + \op{n^{-1}} &\text{ if }\alpha\in\Tcal
    \end{cases}
    \end{equation}
\end{lemma}

\begin{proof}
    Using the Taylor expansion of \(1/{(1-x)}^{2} = 1+2x + O(x^{2})\), we have
    \[\frac{1}{\paren{1-\lev}^{2}} = 1 + 2\lev + \Op{\lev^{2}}.\]
    Thus,
    \begin{equation}
        \label{eq:xizeta}
        \loocv{\alpha} = \underbrace{\frac{1}{n}\sum_{i=1}^{n}\paren{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}^{2}}_{\xi_{\alpha,n}} + \underbrace{\frac{1}{n}\sum_{i=1}^{n}\paren{2\lev + \Op{\lev^{2}}}\paren{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}^{2}}_{\zeta_{\alpha,n}}
    \end{equation}
    Let \(\xi_{\alpha,n}\) and \(\zeta_{\alpha,n}\) denote the first and second terms in (\ref{eq:xizeta}), respectively. Note that
    \begin{align}
        \notag\xi_{\alpha,n} &= \frac{1}{n}\normsq{M_{\alpha}\X\bbeta + M_{\alpha}\e}\\
        \label{line:decomp}&= \frac{1}{n}\paren{\normsq{M_{\alpha}\e} + \normsq{M_{\alpha}\X\bbeta} + 2 \e^{\top}M_{\alpha}\X\bbeta}\\
        \label{line:decomp2}&=\frac{1}{n}\normsq{\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + \frac{1}{n}\normsq{H_{\alpha}\e} + \frac{2}{n}\e^{\top}M_{\alpha}\X\bbeta
        %\label{line:op1}&= \frac{1}{n}\normsq{\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + \op{1}.
    \end{align}
    From here, we emphasize four intermediate steps:
    \begin{enumerate}[i.]
        \item Using Markov's inequality, for \(\varepsilon>0\), \[\prob\set{\normsq{H_{\alpha}\e}\geq n\varepsilon}\leq \frac{\sigma^{2}p_{n}(\alpha)}{n\varepsilon}\to 0\] 
        \[\implies \frac{1}{n}\normsq{H_{\alpha}\e} = o_{\prob}(1).\]
        \item Since \(M_{\alpha}\) is a projection matrix, \(\normsq{M_{\alpha}\X\bbeta}\leq \normsq{\X\bbeta} = O_{\prob}(n)\), so that \[\Ep{\paren{\e^{\top} M_{\alpha}\X\bbeta}^{2}\mid\X} = \frac{4}{n^{2}}\sigma^{2}\normsq{M_{\alpha}\X\bbeta} = o_{\prob}(1).\] Combining the latter with \(\Ep{\e^{\top} M_{\alpha}\X\bbeta\mid\X} = 0\), we obtain that \[\frac{2}{n}\e^{\top}M_{\alpha}\X\bbeta = o_{\prob}(1).\]
        \item Combining i.\ and ii.\ with (\ref{line:decomp2}) yields \[\xi_{\alpha,n} = \frac{1}{n}\normsq{\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + o_{\prob}(1).\] Furthermore, since \(\normsq{\e} = O_{\prob}(n)\), we have that \(\xi_{\alpha,n} = O_{\prob}(1)\).
        \item Finally, since \(0<\lev<1\), \(2\lev + \Op{\lev^{2}}\leq\Op{\max_{i}\lev}\). Thus,
        \[\zeta_{\alpha,n}\leq\Op{\max_{i}\lev}\paren{\frac{1}{n}\sum_{i=1}^{n}\paren{y_{i} - \x_{i\alpha}^{\top}\bbetahat_{\alpha}}^{2}} = \Op{\max_{i}\lev}\xi_{\alpha,n}.\]
        From assumption \textbf{H3}, \(\zeta_{\alpha,n} = o_{\prob}(1)\xi_{\alpha,n} = o_{\prob}(1)\).
    \end{enumerate}
    It follows that 
    \[\loocv{\alpha} = \frac{1}{n}\normsq{e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + o_{\prob}(1) \stackrel{(\mathrm{LLN})}{=} \sigma^{2} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + o_{\prob}(1).\]
    Noting that \(R_{n}(\alpha) = \frac{1}{n}\normsq{M_{\alpha}\X\bbeta} + o_{\prob}(1)\) yields the first case in (\ref{eq:lemmadecomp}).

    If \(\alpha\in\Tcal\), it is easy to see from (\ref{line:decomp}) that \(\xi_{\alpha,n} = 1/n\normsq{M_{\alpha}\e}\), Furthermore,
    \[\zeta_{\alpha,n} = \frac{2}{n}\sigma^{2}p(\alpha) + \op{1},\qquad\textcolor{red}{(?)}\]
    proving the second case.
\end{proof}

\begin{proposition}[Shao, 1993~\cite{shao_1993}]
    Suppose that \(\Tcal\) is non-empty and let \(\alphahat{(1)}\) be the model minimizing \(\loocv{\alpha}\).
    \begin{enumerate}
        \item Under H1, H2, and H3, \[\lim_{n\to\infty}\prob\set{\alphahat{(1)}\in\Tcal^{c}}=0.\]
        \item For \(\alpha\in\Tcal\) with \(\alpha\neq\alpha^{*}\),\[\prob\set{\loocv{\alpha}\leq \loocv{\alpha^{*}}} = \prob\set{2\paren{p(\alpha) - p(\alpha^{*})}\sigma^{2} < \e^{\top}(H_{\alpha} - H_{\alpha^{*}})\e} + \op{1}.\] In particular, if \(\e\sim\Ncal(0_{n}, \sigma^{2}I_n)\), \[\prob\set{\loocv{\alpha}\leq \loocv{\alpha^{*}}} = \prob\set{2k < \chi^{2}(k)} + \op{1} > 0\] for \(k=p(\alpha) - p(\alpha^{*})\).
        \item If \(p(\alpha^*) < p\), \[\lim_{n\to\infty}\prob\set{\alphahat{(1)}=\alpha^{*}}\neq 1.\]
    \end{enumerate}
\end{proposition}

\begin{proof}
    \(\)

    \noindent1.\quad Let \(\alphabar\in \Tcal\) and \(\alphatilde\in\Tcal^{c}\). By, Lemma~\ref{lem:lemmadecomp}, we have that
    \begin{align*}
        \prob\set{\loocv{\alphatilde}\leq \loocv{\alphabar}} &= \prob\Big\{\frac{1}{n}\sigma^{2}p(\alphatilde) + \frac{1}{n}\normsq{M_{\alphatilde}\X\bbeta} + \sigma^{2} + o_{\prob}(1)\\
        &\hspace{2in} \leq \frac{1}{n}\normsq{M_{\alphabar}\e} + \frac{1}{n}\sigma^{2}p(\alphabar) + o_{\prob}(n^{-1})\Big\}\\
        &= \prob\set{\frac{1}{n}\sigma^{2}\paren{p(\alphatilde) - p(\alphabar)} + \sigma^{2} + \frac{1}{n}\normsq{M_{\alphatilde}\X\bbeta} - \frac{1}{n}\normsq{M_{\alphabar}\e}\leq o_{\prob}(1)}
    \end{align*}
    From \textbf{H1}, the latter probability goes to zero as \(n\to\infty\). Therefore, \(\ind{\loocv{\alphatilde}\leq \loocv{\alphabar}} = o_{p}(1)\). We now observe that
    \[\prob\set{\alphahat{(1)}\in\Tcal^{c}} = \Ep{\ind{[\alphahat{(1)}\in\Tcal^{c}]}} = \Ep{\sum_{\alphatilde\in\Tcal^{c}}\prod_{\alpha\in\Acal} \ind{\loocv{\alphatilde}\leq \loocv{\alpha}}} \to 0.\]

    \noindent2.\quad The first part follows from Lemma 2.1 by algebraic manipulation. The second part follows by noting that, if \(\e\sim\Ncal(0_{n}, \sigma^{2}I_{n})\), then \[\frac{\e^{\top}}{\sigma}\paren{H_{\alpha} - H_{\alpha^{*}}}\frac{\e}{\sigma}\sim\chi^{2}\paren{\mathrm{tr}\paren{H_{\alpha}-H_{\alpha^{*}}}}.\]

    \noindent3.\quad It is easy to see that \(p(\alpha^{*}) = p\) if and only if \(\Tcal=\set{\alpha^{*}}\). Thus, if \(p(\alpha^{*}) < p\), there exists \(\alpha\in\Tcal^{c}\) with \(\alpha\neq \alpha^{*}\). The result then follows by part 2 above.
\end{proof}

\begin{corollary}
    Leave-one-out cross-validaton is not consistent for selection. In particular, it overfits with non-vanishing probability.
\end{corollary}

%A subsequent result states that cross-validation is consistent if \(n_{v}/n\to 1\) as \(n\to\infty\), where \(n_{v}\) is the number of validation samples.



\subsection{A General Perspective: Shao, 1997}
\renewcommand{\Acal}{\mathcal{A}_{n}}
\renewcommand{\Tcal}{\mathcal{T}_{n}}

\textcolor{red}{[Brief intro to section]}

%\textcolor{red}{NOTE: For many of the results in Shao (1997), it seems that the fixed design is essential. I've been trying to extend these to a random design, but it's been more difficult than I expected. We can discuss this in our meeting.}

For this section, we allow the number of candidates in \(\Acal\), as well as the candidates \(\alpha\in\Acal\) themselves, to vary with \(n\) (though we assume that both remain finite). To illustrate why this might be useful, we consider two examples from~\cite{shao_1997}:

If we wish to approximate a univariate regression function \(x\mapsto f(x)\) by a polynomial of degree at most \(p_{n}< n\), we may consider the models indexed by \(\Acal:=\set{\alpha_{d}:d\in[p_n]}\), with \(\alpha_{d} = \set{1,\ldots,d}\) and \(f_{\alpha_{d}}(x) = \beta_{0} + \beta_{1}x + \cdots + \beta_{d}x^{d}\). Clearly, the number of candidate models increases as more observations become available.
%\textcolor{red}{This only illustrates the case where the number of candidates increases with \(n\). I can't think of an example where the candidates themselves change with \(n\). It also doesn't make much sense to me.}

The \emph{Generalized Information Criterion} (GIC), defined below, is a generalization of multiple empirical criteria for model selection. Various cross-validation methods can be seen as special cases of the GIC.

\begin{definition}
    We define the GIC loss estimator to be
    \[\Rhat{\lambda_{n}}{\alpha} := \frac{\normsq{\y - \hat{m}(\X)}}{n} + \frac{1}{n}\lambda_{n}\sigmahat p_{n}(\alpha)\quad\text{for }\alpha\in\Acal,\]
    where \(\sigmahat\) is an estimator of \(\sigma^{2}\) and \(\lambda_{n}\) is a sequence of positive real numbers satisfying \(\lambda_{n}\geq 2\) and \(\lambda_{n}/n\to 0\).
\end{definition}

\subsubsection{The case of \(\lambda_{n} \equiv 2\)}

\begin{proposition}[Shao, 1997~\cite{shao_1997}]\label{prop:97prop1}
    Suppose that \(\lambda_{n}=2\) for all \(n\geq 1\) and that \(\sigmahat\) is a consistent estimator of \(\sigma^{2}\). Then,
    \[\Rhat{2}{\alpha} = \begin{cases}
        \frac{1}{n}\normsq{e} + \frac{2}{n}\sigmahat p_{n}(\alpha) - \frac{1}{n}\normsq{H_{\alpha}\e} &\text{if }\alpha\in\Tcal\\[3mm]
        \frac{1}{n}\normsq{e} + \Loss{\alpha} + \op{\Loss{\alpha}} &\text{if }\alpha\in\Tcal^{c}
    \end{cases}\]
\end{proposition}

\begin{theorem}[Shao, 1997~\cite{shao_1997}]\label{thm:97thm1}
    Suppose that H4 holds and that \(\sigmahat\) is consistent for \(\sigma^{2}\).
    \begin{enumerate}
        \item If \(|\Tcal|\leq1\) for all but finitely many \(n\), then \(\alphahat{2}_{n}\) asymptotically loss efficient.
        \item Suppose that \(|\Tcal|> 1\) for all but finitely many \(n\). If there exists a positive integer \(m\) such that \(\Ep{y_{1} - \x_{1}^{\top}\bbeta}^{4m}<\infty\) and 
        \begin{equation}
            \sum_{\alpha\in\Tcal}\frac{1}{\paren{p_{n}(\alpha)}^{m}}\to 0 \quad\text{or}\quad \sum_{\substack{\alpha\in\Tcal,\\ \alpha\neq\alpha^{*}}}\frac{1}{\paren{p_{n}(\alpha) - p_{n}(\alpha^{*})}^{m}},
        \end{equation}
        then \(\alphahat{2}_{n}\) is assymptotically loss efficient.
        \item Suppose that \(|\Tcal|>1\) for all but finitely many \(n\). Suppose, furthermore, that for some constant \(c>2\), 
        \begin{equation}
            \label{eq:conditionQ}
            \liminf_{n\to\infty}\inf_{Q_{n}\in\mathcal{Q}_{n,q}}\prob\set{\normsq{Q_{n}\e} > c\sigma^{2}q} > 0,
        \end{equation}
        where \(\mathcal{Q}_{n,q}\) is the set of all projection matrices of rank \(q\). Then, if \(|\Tcal|\) is bounded or \(\Acal\) is embedded, the condition that
        \begin{equation}
            \label{eq:conditionP}
            p_{n}(\alpha^{*}_{n})\to\infty\quad\text{or}\quad \min_{\substack{\alpha\in\Tcal,\\ \alpha\neq\alpha^{*}}}\paren{p_{n}(\alpha) - p_{n}(\alpha^{*})}\to\infty
        \end{equation}
        is necessary and sufficient for the asymptotic loss efficiency of \(\alphahat{2}_{n}\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \textcolor{red}{The proof for 1.\ is given in the last paragraph of page 226. I don't understand it.}
\end{proof}

Note that condition (\ref{eq:conditionQ}) is satisfied if \(\e\sim\Ncal(0_{n}, \sigma^{2}I_{n})\). Condition (\ref{eq:conditionP}) is satisfied if \(\Acal\) does not contain two correct models with fixed dimension for all but finitely many \(n\).
\begin{corollary}[Shao, 1997]
    If \(\Tcal\) contains exactly one model with fixed dimension for all but finitely many \(n\), then \(\alphahat{2}_{n}\) is consistent.
\end{corollary}

\begin{proof}
    This follows immediately from Theorem~\ref{thm:97thm1} and Proposition~\ref{prop:97prop1}.
\end{proof}

\subsubsection{The case of \(\lambda_{n} \to \infty\)}

\textcolor{red}{The proofs are missing here.}

We now consider the case of the GIC \(\Rhat{n}{\lambda_{n}}\) with \(\lambda\to\infty\) as \(n\to \infty\). Unlike in the previous case, the following results do not require that \(\sigmahat\) be consistent for \(\sigma^{2}\).

\begin{proposition}[Shao, 1997~\cite{shao_1997}]\label{prop:97decomp2}
    Suppose that \(\lambda_{n}=2\) for all \(n\geq 1\) and that \(\sigmahat\) is a consistent estimator of \(\sigma^{2}\). Then,
    \[\Rhat{2}{\alpha} = \begin{cases}
        \frac{1}{n}\normsq{e} + \frac{2}{n}\sigmahat p_{n}(\alpha) - \frac{1}{n}\normsq{H_{\alpha}\e} &\text{if }\alpha\in\Tcal\\[3mm]
        \frac{1}{n}\normsq{e} + \Loss{\alpha} + \frac{1}{n}p_{n}(\lambda_{n}\sigmahat - 2\sigma^{2}) + \op{\Loss{\alpha}} &\text{if }\alpha\in\Tcal^{c}
    \end{cases}\]
\end{proposition}

\begin{theorem}[Shao, 1997~\cite{shao_1997}]\label{thm:97thm2}
    Suppose that \textbf{H4} holds and that
    \begin{equation}\label{eq:310}
        \limsup_{n\to\infty}\sum_{\alpha\in\Tcal}\frac{1}{p_{n}{(\alpha)}^{m}}
    \end{equation}
    for some \(m\) with \(\Ep{e_{i}^{4m}}<\infty\).
    \begin{enumerate}
        \item If \textbf{H1}, \(\lambda_{n}\to\infty\), and \(\lambda_{n}p_{n}/n \to 0\) are satisfied, then \(\Rhat{n}{\lambda_{n}}\) is asymptotically loss efficient.
        \item If there exists \(\alpha_{0}\in\Tcal\) with \(p_{n}(\alpha_{0})\) constant for all but finitely many \(n\), \(\lambda_{n}\to\infty\), and \(\lambda_{n}/n\to 0\), then \(\Rhat{n}{\lambda_{n}}\) is consistent. 
    \end{enumerate}
\end{theorem}

\textbf{Remark:} Condition (\ref{eq:310}) is satisfied whenever \(|\Tcal|\) is bounded or \(\Acal\) is embedded. It implies that 
\[ \max_{\alpha\in\Tcal} \frac{\normsq{H_{\alpha}\e}}{\lambda_{n}\sigmahat p_{n}(\alpha)}\xrightarrow{\prob} 0.\]

\subsubsection{Cross-validation}

\begin{theorem}[Shao, 1997~\cite{shao_1997}]\label{thm:97thm45}
    \begin{enumerate}
        \item If \textbf{H3} holds, then Theorem~\ref{thm:97thm1} applies for the leave-one-out estimator.
        \item Suppose that \textbf{H1}, \textbf{H4}, and (\ref{eq:310}) hold. If the splits are ``balanced'', and \(d\) is chosen so that \(d/n\to1\) and \(p_n/(n-d) \to 0\), then the delete-\(d\) cross-validation estimator is asymptotically loss efficient 
    \end{enumerate}
\end{theorem}

\textcolor{red}{MISSING: Discussion.}


\section{CV for Nonparametric Model Selection}

\subsection{Comparison of Distinct Procedures: Yang, 2007}

Here we consider two regression procedures, denoted \(\delta_{1}\) and \(\delta_{2}\), that yield estimators \(\hat{f}_{n, 1}\) and \(\hat{f}_{n, 2}\) of the regression function stisfying
\begin{equation}\label{eq:regressionmodel}
    y_{i} = f(\x_{i}) + \epsilon_{i}\quad i\in[n],
\end{equation}
for \(\x_{i}\) iid, \(\Ep{\epsilon_{i}\mid \X} \aseq 0\) and \(\Ep{\epsilon_{i}^{2}\mid \X} \stackrel{\text{a.s.}}{<} \infty\).

\begin{definition}
    We say \(\delta_{1}\) is \emph{assymptotically better} than \(\delta_{2}\) under the loss function \(L\) if, for \(0<\varepsilon<1\), there exists \(c_{\varepsilon}>0\) such that
    \[\prob\set{\Loss{\fhat{n}{2}}\geq (1+c_{\epsilon})\Loss{\fhat{n}{2}} }\geq 1-\varepsilon.\]

    Given that \(\delta_{1}\) is assymptotically better than \(\delta_{2}\), we say that a selection procedure is consistent if it selects \(\delta_{1}\) with probability tending to 1 as \(n\to\infty\).
\end{definition}


\subsubsection{Single-split cross-validation (the Hold-out)}

For this section, we assume that the first \(n_1\) elements in \(\data\) are used as a training/estimation sample and the remaining \(n_2\) elements make up the validation sample. We write \(p_{n}\) and \(q_{n}\) for the rates of convergence of the estimators \(\fhat{n}{1}\) and \(\fhat{n}{2}\), respectively. That is,
\[\Op{p_{n}} = \norm{f - \fhat{n}{1}}_{2}\quad\text{and}\quad \Op{q_{n}} = \norm{f- \fhat{n}{2}}_{2}.\]

The hold-out cross-validation method consists in selecting the estimator that minimizes the hold-out loss
\[L_{\mathrm{ho}}(\fhat{n}{j}) = \sum_{i=n_{1}+1}^{n}\paren{y_{i} - \fhat{n}{j}(\x_{i})}^{2}\quad\text{for }j=1,2.\]
The propositions in this section rely on the following conditions:
\begin{itemize}
    \item \textbf{A3.1:} \(\Ep{\epsilon_{i}^{2}\mid \x_{i}}\) is bounded a.s.\ for \(i\in[n]\).
    \item \textbf{A3.2:} There exists \(A_{n}\) such that \(\norm{f-\fhat{n}{j}}_{\infty} = \Op{A_{n}}\) for \(j=1,2\).
    \item \textbf{A3.3:} One procedure is asymptotically better than the other.
    \item \textbf{A3.4:} There exists \(M_{n}\) such that \(\norm{f-\fhat{n}{j}}_{4} / \norm{f-\fhat{n}{j}}_{4} = \op{M_{n}}\) for \(j=1,2\).
\end{itemize}

\begin{theorem}[Yang, 2007~\cite{yang_2007}]\label{prop:yangth1}
    Suppose that \textbf{A3.1}--\textbf{A3.4} hold. Suppose, furthermore, that
    \begin{enumerate}
        \item \(n_{1}\to\infty\)
        \item \(n_{2}\to\infty\)
        \item \(n_{2}M_{n}^{-4} \to \infty\)
        \item \(\sqrt{n_{2}}\max(p_{n_{1}}, q_{n_{1}})\)
    \end{enumerate}
    Then, the hold-out CV procedure is consistent.
\end{theorem}

A very detailed proof of this result is provided in Yang~\cite{yang_2007}, so it will be skipped here.

\subsubsection{Voting cross-validation with multiple splits}

The (theoretical) majority-vote cross-validation method proceeds as follows: for each permutation \(i\mapsto\pi(i)\) of the data, we compute the estimators \(\fhat{n_{1}}{1}\) and \(\fhat{n_{1}}{2}\) using the first \(n_{1}\) data points \((y_{\pi(1)}, \x_{\pi(1)}), \ldots,(y_{\pi(n_1)}, \x_{\pi(n_1)})\) as the training sample and the remaining \(n_{2}=n-n_{1}\) elements as the validation sample. We then find the estimator that minimizes the hold-out loss
\[L_{\pi}(\fhat{n_{1}}{j}) = \sum_{i=n_{1}+1}^{n}\paren{y_{\pi(i)} - \fhat{n_{1}}{j}\paren{\x_{\pi(i)}}}^{2}\qquad \text{for }j=1,2.\]
The chosen estimator is the one favored by the majority of the permutations. More formally, we define 
\[\tau_{\pi} = \ind{L_{\pi}(\fhat{n_{1}}{1}) \leq L_{\pi}(\fhat{n_{1}}{2})}\]
We then define our selection criterion as follows:
\[\hat{f}_{n} = \begin{cases}
    \fhat{n}{1} &\text{if }\sum_{\pi\in\Pi}\tau_{\pi} \geq {n!}/{2},\\[2mm]
    \fhat{n}{2} &\text{otherwise,}
\end{cases}\]
where \(\Pi\) denotes the set of all permutations of \([n]\).

\begin{theorem}[Yang, 2007~\cite{yang_2007}]\label{prop:yangth2}
    Under the conditions of Theorem 4.1 and the condition that the data is iid, the majority-vote cross-validation method is consistent.
\end{theorem}

\begin{proof}
    Suppose that \(\delta_{1}\) is asymptotically better than \(\delta_{2}\). For \(\pi\in \Pi\), we have that
    \[\prob\set{L_{\pi}\paren{\fhat{n_{1}}{1}} \leq L_{\pi}\paren{\fhat{n_{1}}{2}}} = \Ep{\tau_{\pi}} \stackrel{(*)}{=} \Ep{\frac{1}{n!}\sum_{\pi\in\Pi}\tau_{\pi}}.\]
    The equality at \((*)\) follows from the fact that the data are iid, hence exchangeable, and thus the \(\tau_\pi\) are identically distributed. By Theorem~\ref{prop:yangth1}, the right-hand side converges to 1 as \(n\to\infty\). Since the average \(1/n! \sum_{\pi}\tau_{\pi}\) is almost surely at most 1, it follows that \(1/n! \sum_{\pi}\tau_{\pi} \to 1\) in probability, and the majority-vote cross-validation method is consistent.
\end{proof}

The proof of Theorem~\ref{prop:yangth2} does not require using the entire set \(\Pi\) of permutations for the majority vote. In fact, Theorem~\ref{prop:yangth1} establishes that even a single data split suffices for consistency, provided the splitting conditions are met. Moreover, Yang~\cite{yang_2007} presents a counterexample demonstrating that these conditions are not merely sufficient but necessary, hence showing that the number of splits does not affect consistency. In other words, multiple splits in cross-validation cannot rescue an inconsistent single-split procedure. A natural question, then, is: if multiple splits do not improve consistency, what is their benefit? This will be explored in a simulation later on.



\section{Aggregation}
\subsection{Bunea et al., 2007}

As before, we consider independent pairs in \(\data := \set{\paren{y_{i}, \x_{i}}:i\in [n]}\) satisfying (\ref{eq:regressionmodel}). Suppose that we have \(M\) candidate estimators of the regression function, denoted \(\fhat{n}{1}, \fhat{n}{2}, \ldots, \fhat{n}{M}\). Instead of selecting a single estimator, we combine them into an \emph{aggregate} \(\ftilde{\lambdahat{}}\) given by
\[\ftilde{\lambdahat{}} = \sum_{j=1}^{M}\lambdahat{j} \fhat{n}{j},\]
with \(\lambdahat{}:=\paren{\lambdahat{1}, \ldots, \lambdahat{M}}\in \Lambda\subset\R^{M}\) chosen to satisfy
\begin{equation}\label{eq:weights}
    \lambdahat{} = \argmin_{\lambda\in\Lambda}\set{\frac{1}{n}\normsq{y - f_{\lambda}(\x)} - \mathrm{pen}(\lambda)}
\end{equation}
for some penalty \(\mathrm{pen}(\lambda)\) on the coefficients.

\subsubsection{Four types of aggregation}

There are four aggregation schemes considered in Bunea et al.~\cite{bunea_2007}, each of which is characterized by a different set \(\Lambda\) of admissible weights \(\lambdahat{}\):
\begin{itemize}
    \item Model Selection Aggregation (MS): A single estimator is selected. That is,\[\Lambda_{\mathrm{MS}} = \set{\lambda\in\R^{M}:\lambda = \boldsymbol{e}_{j}\text{ for some }j\in[M]}.\]
    \item Linear Aggregation (L): \(\ftilde{\lambdahat{}}\) is chosen among all linear combinations of the estimators. That is, \[\Lambda_{\mathrm{L}} = \R^{M}.\]
    \item Convex Aggregation (C): \(\ftilde{\lambdahat{}}\) is chosen among all convex combinations of the estimators. That is, \[\Lambda_{\mathrm{C}} = \set{\lambda\in\R^{M}:\lambda\geq 0, \sum_{j=1}^{M}\lambda_{j} = 1}.\]
    \item Subset Selection (S): We select and aggregate at most \(D\) estimators from the pool, for a given \(D\leq M\). That is, \[\Lambda_{\mathrm{S}} = \set{\lambda\in\R^{M}:\lambda\text{ has at most \(D\) non-zero entries}}.\]
\end{itemize}

\subsubsection{Evaluating the aggregate}

In an ideal scenario, we would like to select  weights \(\lambda^*\) satisying 
\[\lambda^{*} = \argmin_{\lambda\in\Lambda}\Ep{d\paren{f, \ftilde{\lambda}}}\]
for some distance function \(d\) (e.g., the \(L_{2}\) norm). However, since the true regression function \(f\) is unknown, this approach is clearly not feasible. Another way of constructing an estimator is to minimize its maximum risk on a class of functions \(\Theta\) containing \(f\). That is, we would like to find \(\lambdahat{}\) satisfying 
\[\sup_{f\in\Theta}\E\normsq{f-\ftilde{\lambdahat{}}}_{2} = \inf_{\lambda\in\Lambda}\sup_{f\in\Theta}\E\normsq{f-\ftilde{\lambda}}_{2}.\]
This is known as \emph{minimax} extimation. 
However, once again, there is no obvious way to compute the expectation \(\E\normsq{f-\ftilde{\lambdahat{}}}_{2}\) for an arbitrary \(f\in\Theta\).

For these reasons, we instead adopt the least-squares approach in (\ref{eq:weights}). But how can we know if this approach is any good? We need a tool to evaluate the performance of our aggregate against \emph{any} possible of regression function \(f\). Oracles provide us with such a tool. 

\begin{definition}[adapted from Tsybakov, 2009~\cite{tsybakov_introduction_2009}]
    Suppose that there exists \(\lambda^{*}\in\Lambda\) such that 
    \[\E\normsq{f-\ftilde{\lambda^{*}}}_{2} = \inf_{\lambda\in\Lambda}\E\normsq{f-\ftilde{\lambda^{*}}}_{2}.\]
    The function \(f\mapsto \ftilde{\lambda^{*}}\) is called the oracle of aggregation under \(L_{2}\).

    We say that the aggregate \(\ftilde{\lambdahat{}}\) {mimics} the oracle if
    \begin{equation}\label{eq:oracleineq}
        \E\norm{f - \ftilde{\lambdahat{}}}_{2} \leq \inf_{\lambda\in\Lambda}\E\norm{f - \ftilde{\lambda}}_{2} + \Delta_{n,M}.
    \end{equation}
    for the smalles possible \(\Delta_{n,M}>0\) independent of \(f\).
\end{definition}

In what follows, the goal is to find lower bounds on \(\Delta_{n,M}\) for each of the aggregation schemes. 

\begin{definition}[Tsybakov, 2009~\cite{tsybakov_introduction_2009}]
    For a class of functions \(\Theta\), a sequence \(\set{\psi_{n}}_{n\geq1}\) of positive numbers is called an \emph{optimal rate of convergence} of estimators \(\hat{f}\) on \(\Theta\) under \(L_{2}\) if there exist constants \(c, C>0\) such that
    \begin{align}
        &\limsup_{n\to\infty}\paren{\psi_{n}^{-2}\inf_{\hat{f}}\sup_{f\in\Theta}\Ep{\norm{f - \hat{f}}_{2}^{2}}}\leq C\\
        \text{and}\qquad&\liminf_{n\to\infty}\paren{\psi_{n}^{-2}\inf_{\hat{f}}\sup_{f\in\Theta}\Ep{\norm{f - \hat{f}}_{2}^{2}}}\geq c
    \end{align}
    \textcolor{red}{An estimator \(\hat{f}_{n}\) is said to be \emph{rate-optimal} if 
    \[\sup_{f\in\Theta}\Ep{\norm{f - \hat{f}_{n}}_{2}^{2}} \leq C'\psi_{n}^{2}\]
    for some \(C'>0\). It is called \emph{asymptotically efficient} of \(\Theta\) under \(L_{2}\) if 
    \[\lim_{n\to\infty}\frac{\sup_{f\in\Theta}\E{\normsq{f-\hat{f}_{n}}}}{\inf_{\hat{f}}\sup_{f\in\Theta}\E{\norm{f - \hat{f}}_{2}^{2}}}=1.\]}
\end{definition}

We adapt Theorem 5.1 in~\cite{bunea_2007} to consider exclusively the \(L_{2}\) norm:

\begin{theorem}[Bunea et al., 2007~\cite{bunea_2007}]\label{prop:buneath5.1}
    \textcolor{red}{(Statement of lower bounds)}
    \[\sup_{f_1, \ldots, f_2\in\mathcal{F}_{0}} \inf_{T_n}\sup_{f\in\mathcal{F}_0} \set{\E\normsq{f - T_n}_{2} - \min_{\lambda\in\Lambda}\normsq{f - \ftilde{\lambda}}_{2}} \geq c\psi_{n}\]
    \textcolor{red}{INCOMPLETE SECTION}
\end{theorem}

\subsubsection{A BIC-type penalty}
\begin{definition}
    Let \(M(\lambda) := \norm{\lambda}_{0}\) (i.e., the number of non-zero coefficients in \(\lambda\)). For \(a>0\), we define the Bunea-Tsybakov-Wegkamp \textcolor{red}{(I don't know what to call it)} penalty to be
    \[\penBIC{\lambda} := \frac{2\sigma^{2}}{n}M(\lambda)\paren{1+\frac{2+a}{1+a}\sqrt{2\log\paren{\frac{eM}{M(\lambda)\lor 1}}} + \frac{1+a}{a}\sqbr{2\log\paren{\frac{eM}{M(\lambda)\lor 1}}}}.\]
    This penalty yields the BIC-type least-squares aggregate \(\ftilde{\lambdahat{\mathrm{BIC}}}=:\ftilde{\mathrm{BIC}}\) with 
    \[\lambdahat{BIC}=\argmin_{\lambda\in\R^{M}}\set{\frac{1}{n}\normsq{\y - f_{\lambda}(\x)} - \penBIC{\lambda}}\]
\end{definition}

\begin{theorem}[Bunea et al., 2007~\cite{bunea_2007}]\label{prop:buneath3.1}
    Assume that the \(e_{i}\) are iid \(\Ncal(0, \sigma^{2})\) and that the functions \(f, \fhat{n}{1}, \ldots, \fhat{n}{M}\) are uniformly bounded. Then, for all \(a>0\), \(M\geq 2\), and \(n\geq 1\),
    \[\E\normsq{\ftilde{\mathrm{BIC}} - f}\leq (1+a)\inf_{\lambda\in\R^{M}}\set{\normsq{\ftilde{\lambda} - f} + \frac{\sigma^{2}}{n}\paren{5+\frac{2+3a}{a}\paren{2\log\paren{\frac{eM}{M(\lambda)\lor 1}}}}M(\lambda)} + \frac{6\sigma^{2}{(1+a)}^{2}}{an(e-1)}\]
\end{theorem}

Theorem~\ref{prop:buneath3.1} implies that \(\ftilde{\mathrm{BIC}}\) mimics the oracles for all four aggregation types.

\newpage
\printbibliography{}
\end{document}