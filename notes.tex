\documentclass[12pt, letter paper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,geometry,fancyhdr,graphicx,xcolor}

\geometry{letterpaper, margin=1in}

\usepackage[most]{tcolorbox}
\definecolor{blockgray}{RGB}{240,240,240}

%\newtcolorbox{definition}[1]{colback=blockgray, 
%colframe=black, fonttitle=\bfseries, 
%title=#1, title filled, 


%\newtcolorbox{proposition}[1]{colback=red!5!white,
%colframe=red!75!black,fonttitle=\bfseries,
%title={#1}}

\newtcolorbox[auto counter, number within=section]{proposition}[2][Proposition]{%
  colframe=blue!50!black,
  colback=blue!10,
  coltitle=white,
  fonttitle=\bfseries,
  title=#1~\thetcbcounter \ifx#2\empty\else~(#2)\fi}

\newtcolorbox[auto counter, number within=section]{definition}{%
  fonttitle=\bfseries,
  title=Definition~\thetcbcounter}

% For indicator functions:
\DeclareMathAlphabet{\mathmybb}{U}{bbold}{m}{n}
\newcommand{\1}{\mathmybb{1}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Int}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Borel}[1]{\mathcal{B}\paren{#1}}
\newcommand{\0}{\emptyset}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ep}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\paren}[1]{\left(#1 \right)}
\newcommand{\sqbr}[1]{\left[#1 \right]}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\norm}[1]{\left|\hspace{-1pt}\left|#1 \right|\hspace{-1pt}\right|}
\newcommand{\normsq}[1]{\norm{#1}^{2}}
\newcommand{\field}{\mathcal{F}}
\newcommand{\ind}[1]{\mathmybb{1}_{#1}}
\newcommand{\mudi}{\,\mu\paren{di}}
\newcommand{\nudj}{\,\nu\paren{dj}}
\newcommand{\dmu}{\,d\mu}
\newcommand{\dnu}{\,d\nu}
\newcommand{\data}{\mathcal{D}_{n}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\aseq}{\stackrel{\mathrm{a.s.}}{=}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\x}{\boldsymbol{x}}
\renewcommand{\xi}{\x_{i}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\acvfx}[1]{\gamma_{X}\paren{#1}}
\newcommand{\Cov}[1]{\mathrm{Cov}\paren{#1}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\Loss}[1]{\mathcal{L}_{n}\paren{#1}}
\newcommand{\Rhat}[2]{\hat{R}_{n}^{#1}\paren{#2}}
\newcommand{\alphahat}[1]{\hat{\alpha}^{#1}}

\title{Project: Cross-validation for model selection

(rough draft)}
\author{Diego Urdapilleta de la Parra}

\begin{document}
%\pagestyle{fancy}
%\fancyhead[R]{ID:\@ 260948672}
%\fancyhead[L]{2025/01/26
%
%MATH 545:\@ Assignment 1}
%\setlength{\headheight}{28pt}
\maketitle

\section{Background}
\subsection{Setup and preliminary definitions}
\begin{tcolorbox}[title=Assumptions, fonttitle=\bfseries]
    \begin{align*}
        &\mathbf{H1:}\qquad \liminf_{n\to\infty} \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}>0 \text{ for all }\alpha \in\Acal\\
        &\mathbf{H2:} \qquad \X^{\top}\X = O(n)\quad\text{and}\quad\paren{\X^{\top}\X}^{-1}= O\paren{n^{-1}}\\[2.5mm]
        &\mathbf{H3:}\qquad \lim_{n\to\infty}\max_{i\leq n}h_{ii,\alpha} = 0\\
    \end{align*}
\end{tcolorbox}
    

Let \(\data := \set{\paren{y_{i}, \x_{i}}:i\in [n]}\) be a set of independent data points drawn from a distribution \(\prob_{y, \x}\) for \((y, \x)\in\R^{1+p}\). We treat the \(\x_{i}\) as predictors of the outcome \(y_{i}\), and we assume a linear model
\[\y = \X\bbeta + \e\]
where \(\X = {[\x_{1}\;\x_{2}\;\cdots\;\x_{n}]}^{\top}\in\R^{n\times p}\) is the design matrix, \(\y = \sqbr{y_{1}\; y_{2}\;\cdots\; y_{n}}^{\top}\), and \(\e\) is a mean-zero random vector with \(\Cov{\e} = \sigma_{2}\boldsymbol{I}_{n}\).

\begin{definition}
    We denote the average squared error of \(\hat{m}_{\alpha}\) by
    \[\Loss{\alpha}:=\frac{1}{n}\norm{\Ep{y\mid\X} - \hat{m}_{\alpha}\paren{\X}}^{2}.\]
    Additionally, we write
    \(R_{n}(\alpha):= \Ep{\Loss{\alpha}\mid\X}\).
\end{definition}

\begin{proposition}{}
    If we assume a linear model \(\y = \X\beta + \e\), then
    \[\Loss{\alpha} = \frac{1}{n}\normsq{H_{\alpha}\e} + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\quad\text{and}\quad R_{n}(\alpha) = \frac{1}{n}\sigma^{2}p(\alpha) + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta},\]
    where \(H_{\alpha} = \X_{\alpha}\paren{\X_{\alpha}^{\top}\X_{\alpha}}^{-1}\X_{\alpha}^{\top}\) and \(M_{\alpha}= I_{n} - H_{\alpha}\).
\end{proposition}
\begin{proof}
    First, we have that
    \begin{align*}
        \normsq{\Ep{\y\mid\X} - \hat{m}_{\alpha}(\X)} &= \normsq{\X\bbeta - \X_{\alpha}\bbetahat_{\alpha}}\\
        &= \normsq{\X\bbeta - H_{\alpha}\paren{\X\bbeta + \e}}\\
        &= \normsq{M_{\alpha}\X\bbeta - H_{\alpha}\e}.
    \end{align*}
    Notice that \(M_{\alpha}\X\bbeta\) and \(H_{\alpha}\e\) are orthogonal:
    \[\e^{\top}H_{\alpha}M_{\alpha}\X\bbeta  = \e^{\top}H_{\alpha}\paren{I_{n} - H_{\alpha}}\X\bbeta = \e^{\top}H_{\alpha}\X\bbeta - \e^{\top}H_{\alpha}\X\bbeta = 0.\]
    Hence, by the Pythagorean theorem, the first part is satisfied.

    For the second part, we note that \(\Ep{\normsq{H_{\alpha}\e}\mid\X} = \sigma^{2}p(\alpha)\) by the ``trace trick,'' where \(p(\alpha)\) denotes the size of model \(\alpha\).
\end{proof}

\begin{proposition}{}
    Suppose that the set of correct candidate models \(\Acal_{c}\subset\Acal\) is non-empty, and let \(\alpha^{*}\) be the smallest correct model in \(\Acal_{c}\). Then, \(\alpha^{*}\) minimizes \(R_{n}(\alpha)\) over \(\alpha\in\Acal\).
\end{proposition}
\begin{proof}
    Let \(\alpha\in\Acal\) be arbitrary and suppose that \(\alpha\in\Acal_{c}\). Then, \(\X_{\alpha}\bbeta_{\alpha} = \X\bbeta\) and \(p(\alpha^{*})\leq p(\alpha)\). Thus,
    \begin{align*}
        R_{n}(\alpha) &= \frac{1}{n}\sigma^{2}p(\alpha) + \frac{1}{n}\normsq{M_{\alpha}\X\bbeta}\\
        &= \frac{1}{n} \sigma^{2}p(\alpha) + \frac{1}{n}\underbrace{\normsq{M_{\alpha}\X_{\alpha}\bbeta_{\alpha}}}_{0}\\
        &= \frac{1}{n}\sigma^{2}p(\alpha)\; \geq \; \frac{1}{n}\sigma^{2}p(\alpha^{*}) = R_{n}(\alpha^{*}).
    \end{align*}
    Now suppose that \(\alpha\in\Acal_{w}\). If \(p(\alpha)\geq p(\alpha^{*})\), the result follows by assumption H1. If \(p(\alpha)\geq p(\alpha^{*})\), then \ldots \textcolor{red}{MISSING.}
\end{proof}

\section{Leave-One-Out CV}
\begin{definition}
    The LOOCV estimator of \(R_{n}(\alpha)\) is 
    \[\Rhat{(1)}{\alpha}:= \frac{1}{n}\sum_{i=1}^{n}\paren{\frac{y_{i}-\x_{i\alpha}^{\top}\bbetahat_{\alpha}}{1-h_{ii,\alpha}}}^{2}\]
\end{definition}

\begin{proposition}[Lemma]{{Shao, 1993}}
    \[\Rhat{(1)}{\alpha} = \begin{cases}
        \frac{1}{n}\normsq{\e} + \frac{2}{n}\sigma^{2}p(\alpha) - \normsq{H_{\alpha}\e} + o_{\prob}(n^{-1}) &\text{ if }\alpha\in\Acal_{c}\\[1mm]
        R_{n}(\alpha) + o_{\prob}(1) &\text{otherwise}
    \end{cases}\]
\end{proposition}

A corollary of this Lemma is that \(\Rhat{(1)}{\alpha}\to_{\prob}\sigma^{2}\leftarrow_{\prob}R_{n}(\alpha)\).

\begin{proposition}{{Shao, 1993}}
    Let \(\alphahat{(1)}\) be the model minimizing \(\Rhat{(1)}{\alpha}\).
    \begin{enumerate}
        \item Under H1, H2, and H3, \[\lim_{n\to\infty}\prob\paren{\alphahat{(1)}\in\Acal_{w}}=0.\]
        \item If \(p(\alpha) \neq p\), \[\lim_{n\to\infty}\prob\paren{\alphahat{(1)}=\alpha^{*}}\neq 1\]
        \item For \(\alpha\in\Acal_{c}\) with \(\alpha\neq\alpha^{*}\),\[\prob\paren{\Rhat{(1)}{\alpha}\leq \Rhat{(1)}{\alpha^{*}}} = \prob\paren{2\paren{p(\alpha) - p(\alpha^{*})}\sigma^{2} < \e^{\top}(H_{\alpha} - H_{\alpha^{*}})\e} + o_{\prob}(1).\] If \(\e\sim\N(0_{n}, \sigma^{2}I_n)\), \[\prob\paren{\Rhat{(1)}{\alpha}\leq \Rhat{(1)}{\alpha^{*}}} = \prob\paren{2k < \chi^{2}(k)} + o_{\prob}(1) \] for \(k=p(\alpha) - p(\alpha^{*})\).
    \end{enumerate}
\end{proposition}

\begin{proposition}[Corollary]{}
    LOOCV overfits with non-zero probability asymptotically.
\end{proposition}

\end{document}